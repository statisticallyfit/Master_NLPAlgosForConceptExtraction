\section{BERT} \label{sec:BERT}

\subsection{Problem with ELMo} \label{sec:ProblemWithELMo}

\subsection{Motivation for BERT} \label{sec:MotivationForBERT}

Language models have been effective at sentence-level nlp tasks like natural language inference and paraphrasing, which predict sentence relationships, and also token-level tasks like \nameref{nlptask:namedentityrecognitionNER} and \nameref{nlptask:questionansweringQA}, where a fine-grained approach is needed (Devlin et al., 2019). 

 proposing   BERT:BidirectionalEncoderRepresentations   fromTransformers.BERT alleviates the previously mentioned unidi-rectionality  constraint  by  using  a  “masked  lan-guage  model”  (MLM)  pre-training  objective

``BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers" (Devlin et al., 2019).


\subsection{Describing BERT} \label{sec:DescribingBERT}

% MASKED LANG MODEL


% Papers here; 
% What does bert look at?

% Does bert make any sense?

% Three-way comparison: How contextual are ELMO, BERT< GPT-2 ?? anistropy statements