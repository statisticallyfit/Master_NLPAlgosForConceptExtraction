\section{APPENDIX: Glossary of NLP Tasks} \label{app:Appendix_NLPTasks}

Most of these definitions are from Collobert et al. (2011). 


\subsection{semantic parsing (SP)} \label{nlptask:semanticparsingSP}

\textbf{Semantic parsing (SP)} converts a natural language representation into machine-understanding form. Types include \nameref{nlptask:machinetranslationMT} and \nameref{nlptask:questionansweringQA}.

\subsection{machine translation (MT)} \label{nlptask:machinetranslationMT}

\textbf{Machine translation (MT)} task translates an input text in a given language to a target language. There are many population neural translation models like the \nameref{sec:Seq2Seq} and \nameref{sec:Transformer} and \nameref{BERT} that use the \hyperref[sec:AttentionMechanism][attention mechanism] to account for contextual meaning across a sentence and not just translate word by word. 


\subsection{question answering (QA)} \label{nlptask:questionansweringQA}

\textbf{Question answering (QA)} is a task for machines to answer questions posed by humans in a natural language. 


\subsection{semantic role labeling (SRL)} \label{nlptask:semanticrolelabelingSRL}

Also called ``shallow" \nameref{nlptask:semanticparsingSP} , \textbf{semantic role labeling (SRL)} is often described as answering the question ``Who did what to whom." It tries to give a semantic role or tag to a syntactic constituent of a sentence (Collobert et al., 2011). It can assign labels to words or phrases in a sentence to indicate their semantic role in that particular sentence. Examples of semantic roles are \emph{agent, goal} or \emph{result}. 

Specifically, it detects semantics associated to a syntactic sentence feature like predicate or verb and then assigns them semantic roles. 
\begin{itemize}
    \item \textbf{Example Input Sentence: } ``Mary sold the book to John."
    
    \item \textbf{Example Output: } for the verb $\Big[ \text{"to sell"}\Big]_\text{predicate}$; for the noun or argument $\Big[ \text{"Mary"}\Big]_\text{agent}$; for the noun $\Big[ \text{"the book"}\Big]_\text{goods (theme)}$; for the noun $\Big[ \text{"John"}\Big]_\text{recipient}$.
\end{itemize}

SRL can give multiple labels depending on the usage of the syntactic constituent in the sentence. 



\subsection{named entity recognition (NER)} \label{nlptask:namedentityrecognitionNER}

\textbf{Named entity recognition (NER)} is a kind of information extraction task that labels known entities in text into categories like ``person" or ``location". 

\subsection{part of speech tagging (POS)} \label{nlptask:postagging}

\textbf{Part-of-speech tagging (POS)} is the process of labeling each word in a sentence with its part of speech. Every word token is labeled with a tag that identifies its syntactic role (noun, verb, advergb, adjective, ...).  

From (Mohler, 2019): 

\begin{itemize}
    \item \textbf{Example Input Sentence: } ``The tall man is going to quickly walk under the ladder."
    
    \item \textbf{Example Output: } $\Big[ \text{"man"}\Big]_\text{Noun}$, \ $\Big[ \text{"walk"}\Big]_\text{Verb}$, \ $\Big[ \text{"ladder"}\Big]_\text{Noun}$, \ $\Big[ \text{"quickly"}\Big]_\text{Adverb}$ and so on. 
\end{itemize}




\subsection{chunking} \label{nlptask:chunking}

\textbf{Chunking} labels entire pieces of a sentence with tags that indicate their part of speech or syntactic role. For example a phrase can be labeled \emph{noun phrase (NP)}, or \emph{verb phrase (VP)} or even \emph{begin-chunk (B-NP)} and \emph{inside-chunk (I-NP)}. Each \emph{phrase} token is assigned one distinct part of speech tag. 

Chunking operates on the \emph{phrase level} while \nameref{nlptask:postagging} operates on the \emph{word level}. 

From (Mohler, 2019): 
\begin{itemize}
    \item \textbf{Example Input Sentence: } ``The tall man is going to quickly walk under the ladder."
    
    \item \textbf{Example Output: } $\Big[ \text{"the tall man"}\Big]_\text{Noun Phrase}$, \ $\Big[ \text{"is going to quickly walk"}\Big]_\text{Verb Phrase}$, \\ $\Big[ \text{"under the ladder"}\Big]_\text{Preopositional Phrase}$.
\end{itemize}


\subsection{sentiment analysis (SA)} \label{nlptask:sentimentanalysisSA}




\subsection{word sense disambiguation (WSD)} \label{nlptask:wordsensedisambiguatioNWSD}

\textbf{Word sense disambiguation (WSD)} identifies the correct word usage from a collection of senses. For a sentences containing \hyperref[sec:Polysemy]{polysemous words}, models use contextual evidence to determine the correct word sense. 



\subsection{lexical substitution} \label{nlptask:lexicalsubstitution}

\textbf{Lexical substitution} substitutes a word given its contextual meaning. For example, the word ``bright" in the phrase ``bright child" can be replaced with ``smart" or ``gifted" rather than ``shining" (Brazinkas et al., 2018). 

\subsection{entailment recognition} \label{nlptask:entailmentrecognition}

The \textbf{entailment recognition} task is a kind of lexical entailment task or hyponymy detection. Like in \nameref{nlptask:wordsimilarity}, context is not given, only a pair of words, where the task is to predict if the first word $w_1$ entails the second one $w_2$. For (``kiwi", ``fruit"), the task would be to confirm that ``kiwi" entails ``fruit" since it is a hyponym. Similarly, ``spoon" entails ``cutlery". 


\subsection{textual entailment (TE)} \label{nlptask:textualentailmentTE}

\subsection{entailment directionality prediction} \label{nlptask:entailmentdirectionalityprediction}

Given a pair of words, the \textbf{entailment directionality prediction} task must predict if the previous word entails the next one, or vice versa. It is known that entailment holds for the given word pair and only its directionality is being predicted (Brazinkas et al., 2018). 


\subsection{word similarity} \label{nlptask:wordsimilarity}

\subsection{word analogy} \label{nlptask:wordanalogy}

\subsection{coreference resolution (CR)} \label{nlptask:coreferenceresolutionCR}

\subsection{sequence labeling (SL)} \label{nlptask:sequencelabelingSL}

\subsection{semantic textual similarity (STS)} \label{nlptask:semantictextualsimilaritySTS}




%%%%%
text summarization (TS)
semantic dependency parsing (SDP)

tokenization/segmentation (ml)
sentence chaining

entity typing (ET)
entity type recognition (ETR)
entity extraction
entity disambiguation (ED)
named entity disambiguation (NED)


subword tokenization (ml)
sentencepiece tokenization (ml)
