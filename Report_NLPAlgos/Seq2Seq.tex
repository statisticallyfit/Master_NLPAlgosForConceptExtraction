\section{Sequence To Sequence Model}

A \textbf{sequence-to-sequence (seq-to-seq) model} is often used in natural language processing for machine translation. It takes a sequence of items such as words and outputs another sequence of items. It uses an \textbf{encoder} that processes the inputs, squashes this information into a \textbf{context vector} which the seq-to-seq model sends to a \textbf{decoder} that outputs a target sentence one word at a time, using the context vector (Alammar, 2018). Commonly, the encoder and decoder are RNNs such as LSTMs or GRUs (described in detail in Appendices).


\subsection{Describing Seq-To-Seq Model}

The key feature in the seq-to-seq model different from a recurrent neural network (RNN) is the \textbf{context vector}, or the final hidden state of the encoder. When the encoder processes the input sequence $\overrightarrow{x} = \{x_1, ..., x_T \}$ of individual words $x_t$ in the input sentence $\overrightarrow{x}$, the information is squashed into a \emph{fixed-length} context vector. Formally, a gated recurrent unit (GRU) as the encoder would output a hidden state given a previous hidden state and the current input: 
$$
h_t = \text{EncoderGRU} \Big( x_t, h_{t-1} \Big)
$$ 
where the context vector named $z$ equals the last hidden state: $z = h_T$. 
\newline 
The context vector is then passed to the decoder along with a target token $y_t$ and previous decoder hidden state $s_{t-1}$ to return a current hidden state, $s_t$:
$$
s_t = \text{DecoderGRU} \Big( y_t, s_{t-1}, z \Big)
$$
The context vector $z$ does not have a time step $t$ subscript, meaning this same context vector from the encoder is reused each time step in the decoder. 


\subsection{Problem with Seq-To-Seq Models}

However, compressing the inputs into such a \textbf{fixed-length} vector leads to a \textbf{long-term dependency problem} since only the last hidden state of the encoder is used. Thus, the seq-to-seq model becomes incapable of memory, similar to RNNs (see appendix B). 

\subsection{The Attention Mechanism}

The \textbf{attention mechanism} was proposed in neural machine translation (NMT) task to memorize longer sentences by ``selectively focusing on parts of the source sentence" as required (Luong et al., 2015). Instead of creating a single context vector $z$ from the encoder's last hidden state $h_T$, the attention architecture creates a context vector for each input word or timestep $t$, reducing the information compression problem. This means the attention mechanism uses all the hidden states generated by the encoder as inputs for the decoding process. For each decoder output, the attention mechanism ``selectively picks out specific elements from the [input] sequence to produce the [decoder] output" (Loye, 2019). This essentially creates links between the context vector and entire source input. 

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{imgs/attention.png}
\caption{\footnotesize Attention Mechanism: How words are considered for contextual evidence. From \emph{Intuitive Understanding of Seq2seq model and Attention Mechanism in Deep Learning}, by Medium, 2019. \url{https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e}. Copyright n.d by n.d.}
\end{figure}

\subsection{Seq-To-Seq Model Using Attention}

The key components of a seq-to-seq model with attention are its attention forward pass, which computes attention scores, and decoder forward pass, which outputs the context vector. 

\subsubsection{Forward Pass of Attention}

The attention mechanism is considered a layer in the seq-to-seq model with a forward pass that updates parameters. The steps for the forward pass to calculate attention scores $\alpha_{ij}$ is as follows: 
\begin{enumerate}
    \item First, an \textbf{alignment model} $a$ is used to calculate \textbf{energy scores} $e_{ij}$ that measure how well the ``inputs around position $j$ and the output at position $i$ match" (Bahdanau et al., 2016). The energy scores are weights specifying how much of the decoder hidden state $s_{i-1}$ and the encoder hidden state $h_j$ of the source sentence should be considered for each output (Ta-Chun, 2018; Bahdanau et al., 2016). 
    $$
    e_{ij} = a(s_{i-1}, h_j)
    $$ 
    
    \item Next, the energy score $e_{ij}$ is passed through a feed-forward neural network and softmax to calculate the attention scores, $\alpha_{ij}$:
    $$
    \alpha_{ij} = \frac{\exp{(e_{ij})} } { \sum_{k=1}^{T_x} \exp{(e_{ik)}} }
    $$
    Transforming the energy scores via softmax is done to ensure the attention vector $\overrightarrow{\alpha} = \Big \{ \alpha_{ij} \Big \}$ has values normalized between $0$ and $1$. 
\end{enumerate}

\begin{center}
    \pythonCodeFile{code/tut3_AttentionClass.py}
\end{center}

% TODO: line wrapping

\subsubsection{Forward Pass of Decoder}

Once the attention scores have been calculated, the decoder can calculate the context vector, $c_i$, which is a sum of encoder hidden states $h_j$ weighted by attention scores $\overrightarrow{\alpha} = \Big \{ \alpha_{ij} \Big \}$ (Ta-Chun, 2018): 
$$
c_i = \sum_{j=1}^{T_x} \alpha_{ij} \cdot h_j
$$

Intuitively, the context vector is an \textbf{expectation}. The attention score $\alpha_{ij}$ is the probability that target word $y_t$ is aligned to (or translated from) an input word $x_j$. Then the $i$-th context vector $c_i$ is the expected hidden state over all hidden states $h_j$ with probabilities $\alpha_{ij}$, where these corresponding energies $e_{ij}$) quantify the importance of encoder hidden state $h_j$ with respect to the previous decoder hidden state $s_{i-1}$ in deciding the next decoder state $s_i$ for generating target word $y_t$. 
\newline Effectively, this is an attention mechanism within the decoder. By this construction, we can relieve the encoder of the burden of encoding all source sentence information into a fixed-length vector, so that information can be spread out through the hidden state sequence $\overrightarrow{h} = \Big \{ h_1,...,h_T\Big \}$ and retrieved judiciously by the decoder (Trevett, 2020). 

\begin{center}
    \pythonCodeFile{code/tut3_DecoderClass.py}
\end{center}


\subsubsection{Forward Pass of Seq-To-Seq Model}

Finally, the seq-to-seq model can use the encoder, decoder and attention in conjunction. Its forward pass is (Trevett, 2020): 
\begin{enumerate}
    \item Create an output tensor to hold predicted words $\hat{y}$
    
    \item Pass the source sequence $\overrightarrow{x} = \Big \{ x_1, ..., x_T \Big \}$ into the encoder to receive the contexts $\overrightarrow{z}$ alongside the hidden states $\overrightarrow{h} = \Big \{ h_1, ..., h_T \Big \}$.
    
    \item Set equal the initial decoder hidden state $s_0$ and last encoder hidden state $h_T$.
    
    \item Decode within a loop: insert the target token $y_t$ and previous hidden state $s_t$ and all encoder outputs $\overrightarrow{h}$ into the decoder to get a prediction $\hat{y}_{t+1}$ and new hidden state $s_t$.
    
    
\end{enumerate}


\begin{center}
    \pythonCodeFile{code/tut3_Seq2Seq.py}
\end{center}